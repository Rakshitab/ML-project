{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8imJSu6WdGP",
        "outputId": "ef51d09a-54b9-48ec-fe9d-1fdf832dcff6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: easyfsl in /usr/local/lib/python3.10/dist-packages (1.5.0)\n",
            "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from easyfsl) (3.7.1)\n",
            "Requirement already satisfied: pandas>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from easyfsl) (2.2.2)\n",
            "Requirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from easyfsl) (2.4.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from easyfsl) (0.19.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from easyfsl) (4.66.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->easyfsl) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->easyfsl) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->easyfsl) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->easyfsl) (1.4.7)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->easyfsl) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->easyfsl) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->easyfsl) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->easyfsl) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->easyfsl) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.5.0->easyfsl) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.5.0->easyfsl) (2024.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->easyfsl) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->easyfsl) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->easyfsl) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->easyfsl) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->easyfsl) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->easyfsl) (2024.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->easyfsl) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.5.0->easyfsl) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.5.0->easyfsl) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install easyfsl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NfRncAIbXMlY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d51c6979-a3f0-427c-d609-a6b856eb796b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import Omniglot\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "image_size = 28\n",
        "\n",
        "\n",
        "train_set = Omniglot(\n",
        "    root=\"./data\",\n",
        "    background=True,\n",
        "    transform=transforms.Compose(\n",
        "        [\n",
        "            transforms.Grayscale(num_output_channels=3),\n",
        "            transforms.RandomResizedCrop(image_size),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "        ]\n",
        "    ),\n",
        "    download=True,\n",
        ")\n",
        "test_set = Omniglot(\n",
        "    root=\"./data\",\n",
        "    background=False,\n",
        "    transform=transforms.Compose(\n",
        "        [\n",
        "            transforms.Grayscale(num_output_channels=3),\n",
        "            transforms.Resize([int(image_size * 1.15), int(image_size * 1.15)]),\n",
        "            transforms.CenterCrop(image_size),\n",
        "            transforms.ToTensor(),\n",
        "        ]\n",
        "    ),\n",
        "    download=True,\n",
        ")\n",
        "class SiameseNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SiameseNetwork, self).__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, kernel_size=10), nn.ReLU(), nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=7), nn.ReLU(), nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(128, 128, kernel_size=4), nn.ReLU(), nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.conv4 = nn.Sequential(\n",
        "            nn.Conv2d(128, 256, kernel_size=4), nn.ReLU()\n",
        "        )\n",
        "        self.fc1 = nn.Sequential(nn.Linear(256*6*6, 4096), nn.Sigmoid())\n",
        "        self.fc2 = nn.Sequential(nn.Linear(4096, 1), nn.Sigmoid())  # Add sigmoid activation function\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        out1 = self.conv1(x1)\n",
        "        out1 = self.conv2(out1)\n",
        "        out1 = self.conv3(out1)\n",
        "        out1 = self.conv4(out1)\n",
        "        out1 = out1.view(out1.size()[0], -1)\n",
        "        out1 = self.fc1(out1)\n",
        "\n",
        "        out2 = self.conv1(x2)\n",
        "        out2 = self.conv2(out2)\n",
        "        out2 = self.conv3(out2)\n",
        "        out2 = self.conv4(out2)\n",
        "        out2 = out2.view(out2.size()[0], -1)\n",
        "        out2 = self.fc1(out2)\n",
        "\n",
        "        # Euclidean distance\n",
        "        distance = torch.abs(out1 - out2)\n",
        "        out = self.fc2(distance)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GbKvy6Rtlsyd"
      },
      "outputs": [],
      "source": [
        "def train_siamese_network(model, train_loader, criterion, optimizer, epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for i, (img1, img2, label) in enumerate(train_loader):\n",
        "            img1, img2, label = img1, img2, label\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(img1, img2)\n",
        "            loss = criterion(output.squeeze(), label)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-wooAzWleMV"
      },
      "outputs": [],
      "source": [
        "def test_siamese_network(model, test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for img1, img2, label in test_loader:\n",
        "            img1, img2, label = img1, img2, label\n",
        "            output = model(img1, img2)\n",
        "            predicted = torch.round(output)\n",
        "            total += label.size(0)\n",
        "            correct += (predicted == label).sum().item()\n",
        "    accuracy = correct / total\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UL8cINvFlb_M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab4ec377-5d5a-4512-8c8b-4ff670eeb2e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting GFLOPS calculation...\n",
            "Performing forward pass...\n",
            "Conv2d Layer FLOPs: 1527696\n",
            "Conv2d Layer FLOPs: 1527696\n",
            "Linear Layer FLOPs: 169744\n",
            "Linear Layer FLOPs: 169744\n",
            "Total FLOPs: 1697440\n",
            "GFLOPS: 0.0017\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class SiameseNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SiameseNetwork, self).__init__()\n",
        "        # Example layers, adjust as necessary\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1)\n",
        "        self.fc1 = nn.Linear(16 * 103 * 103, 1)  # Adjust based on output size\n",
        "\n",
        "    def forward(self, img1, img2):\n",
        "        # Implement forward pass logic\n",
        "        x1 = self.conv1(img1)\n",
        "        x2 = self.conv1(img2)\n",
        "        x1 = x1.view(x1.size(0), -1)\n",
        "        x2 = x2.view(x2.size(0), -1)\n",
        "        output = self.fc1(x1) - self.fc1(x2)  # Example output logic\n",
        "        return output\n",
        "\n",
        "def calculate_gflops(model, input_size, batch_size=1):\n",
        "    print(\"Starting GFLOPS calculation...\")\n",
        "    flops = 0\n",
        "    input_tensor1 = torch.randn(batch_size, *input_size).to(next(model.parameters()).device)\n",
        "    input_tensor2 = torch.randn(batch_size, *input_size).to(next(model.parameters()).device)\n",
        "\n",
        "    # Hook functions\n",
        "    def conv2d_hook(module, input, output):\n",
        "        # Retrieve batch size from the input shape\n",
        "        batch_size = input[0].shape[0]\n",
        "        out_channels = output.shape[1]\n",
        "        out_h, out_w = output.shape[2:4]\n",
        "        kernel_h, kernel_w = module.kernel_size\n",
        "        in_channels = module.in_channels\n",
        "        flops = batch_size * out_channels * out_h * out_w * (kernel_h * kernel_w * in_channels)\n",
        "        module.__flops__ = flops\n",
        "        print(f\"Conv2d Layer FLOPs: {flops}\")\n",
        "\n",
        "    def linear_hook(module, input, output):\n",
        "        # Retrieve batch size from the input shape\n",
        "        batch_size = input[0].shape[0]\n",
        "        flops = batch_size * module.in_features * module.out_features\n",
        "        module.__flops__ = flops\n",
        "        print(f\"Linear Layer FLOPs: {flops}\")\n",
        "\n",
        "    # Attach hooks\n",
        "    hooks = []\n",
        "    for module in model.modules():\n",
        "        if isinstance(module, nn.Conv2d):\n",
        "            hooks.append(module.register_forward_hook(conv2d_hook))\n",
        "        elif isinstance(module, nn.Linear):\n",
        "            hooks.append(module.register_forward_hook(linear_hook))\n",
        "\n",
        "    # Forward pass to trigger hooks\n",
        "    print(\"Performing forward pass...\")\n",
        "    model(input_tensor1, input_tensor2)\n",
        "\n",
        "    # Sum up the FLOPs\n",
        "    for module in model.modules():\n",
        "        if hasattr(module, '__flops__'):\n",
        "            flops += module.__flops__\n",
        "\n",
        "    # Remove hooks after use\n",
        "    for hook in hooks:\n",
        "        hook.remove()\n",
        "\n",
        "    # Convert FLOPs to GFLOPS\n",
        "    gflops = flops / 1e9\n",
        "    print(f\"Total FLOPs: {flops}\")\n",
        "    return gflops\n",
        "\n",
        "# Example Usage\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = SiameseNetwork().to(device)\n",
        "input_size = (1, 105, 105)  # Input shape\n",
        "gflops = calculate_gflops(model, input_size, batch_size=1)\n",
        "print(f\"GFLOPS: {gflops:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_model_size(model):\n",
        "    # Total parameters count\n",
        "    model_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    model_size_mb = model_parameters * 4 / (1024 ** 2)  # 4 bytes per parameter (float32)\n",
        "    print(f\"Model size: {model_size_mb:.2f} MB\")\n",
        "    return model_size_mb\n",
        "\n",
        "# Example usage\n",
        "model_size = calculate_model_size(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4cSOqEZ_wpm",
        "outputId": "944ddf6f-4231-46c7-d20a-8a4db60d8e10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model size: 0.65 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def calculate_inference_time(model, input_size, device, num_iterations=100):\n",
        "    model.eval()\n",
        "    inputs1 = torch.randn(1, *input_size).to(device)\n",
        "    inputs2 = torch.randn(1, *input_size).to(device)\n",
        "\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        for _ in range(num_iterations):\n",
        "            _ = model(inputs1, inputs2)\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    avg_inference_time = total_time / num_iterations\n",
        "    print(f\"Average inference time per image pair: {avg_inference_time:.6f} seconds\")\n",
        "    return avg_inference_time\n",
        "\n",
        "# Example usage\n",
        "avg_time = calculate_inference_time(model, input_size, device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBgQLE8F_9MB",
        "outputId": "0340cceb-43af-4b67-8344-9bf364840850"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average inference time per image pair: 0.000281 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_accuracy(model, dataloader, device):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for data in dataloader:\n",
        "            # Adjust the unpacking based on your dataset\n",
        "            if len(data) == 3:\n",
        "                inputs1, inputs2, labels = data\n",
        "            elif len(data) == 2:\n",
        "                inputs1, labels = data\n",
        "                # If you're using a single input for comparison, you'll need to adjust how you handle this\n",
        "                inputs2 = inputs1  # Example: using the same input for the sake of demonstration\n",
        "\n",
        "            inputs1, inputs2, labels = inputs1.to(device), inputs2.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs1, inputs2)\n",
        "            predicted = (outputs > 0.5).float()  # Threshold for binary classification\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = correct / total\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    return accuracy\n",
        "\n",
        "# Example usage (ensure validation_loader is defined and populated)\n",
        "# accuracy = calculate_accuracy(model, validation_loader, device)\n"
      ],
      "metadata": {
        "id": "-XqfYYgy_00t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you are using a custom dataset\n",
        "train_dataset = Omniglot(root='data', background=True, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Create a validation dataset and loader (you can split the original dataset)\n",
        "validation_dataset = Omniglot(root='data', background=False, download=True, transform=transform)\n",
        "validation_loader = DataLoader(validation_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Now you can use this validation_loader in the accuracy function\n",
        "accuracy = calculate_accuracy(model, validation_loader, device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtoZjMOZGGEd",
        "outputId": "6933014c-7be6-4f92-d19f-ff3ac0874528"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Accuracy: 0.0486\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have defined the training DataLoader somewhere\n",
        "# We'll split the Omniglot dataset into training and validation sets\n",
        "\n",
        "from torchvision.datasets import Omniglot\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "# Define the transformation (same as training)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.Resize((105, 105)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Load the Omniglot dataset\n",
        "full_dataset = Omniglot(root='./data', transform=transform, download=True)\n",
        "\n",
        "# Split the dataset into training and validation (e.g., 80% train, 20% validation)\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "validation_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Now you can use `validation_loader` in your accuracy function\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOGxtDRHFhqJ",
        "outputId": "7f68c031-194c-458b-9e27-55c1bd88940a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}